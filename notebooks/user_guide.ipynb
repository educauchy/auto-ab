{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User guide \"How to use auto_ab library\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, yaml, os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append(str('../'))\n",
    "from auto_ab import ABTest, Splitter, VarianceReduction, Graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading config file\n",
    "\n",
    "Config file is in *yaml* format and is located in the root of the library.\n",
    "Later in file config is available via *config* variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    project_dir = os.path.dirname(os.path.abspath(''))\n",
    "    config_file = os.path.join(project_dir, 'config.yaml')\n",
    "    with open (config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except yaml.YAMLError as exc:\n",
    "    print(exc)\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print('Error reading the config file')\n",
    "    sys.exit(1)\n",
    "    \n",
    "\n",
    "gf = Graphics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "\n",
    "- **sex, married, country** — independent of experiment features\n",
    "- **weight_now, noise_now** — features during experiment\n",
    "- **height_now** — target during experiment if continuous metric\n",
    "- **clicks_now, sessions_now** — numerator and denominator of ratio metric during experiment\n",
    "\n",
    "- **weight_prev, noise_prev** — features before experiment\n",
    "- **height_prev** — target before experiment if continuous metric\n",
    "- **clicks_prev, sessions_prev** — numerator and denominator of ratio metric before experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(project_dir, 'data/internal/guide/data.csv'))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 stages in the experiment\n",
    "1. **Preparation to the experiment** — calculate best MDE\n",
    "2. **Experiment** — log data during experiment (this stage is not currently developed in the library)\n",
    "3. **A/A test** — A/A test, or splitter test\n",
    "4. **Variance reduction** — reduce variance of given during the experiment metrics\n",
    "5. **A/B test analysis** — actual A/B test by certain statistical technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Continuous metric\n",
    "# 1.1. Preparation to the experiment\n",
    "## Loading dataset\n",
    "\n",
    "- **sex, married, country, weight_prev, noise_prev** — features\n",
    "- **height_prev** — target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data[['id', 'sex', 'married', 'country', 'weight_prev', 'noise_prev', 'height_prev']]\n",
    "data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of splitter\n",
    "\n",
    "If you are going to run MDE simulation, **split_rate** parameter can be omitted as it will be placed to the splitter during the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = Splitter(split_rate=config['splitter']['split_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom splitter\n",
    "\n",
    "We can pass custom splitter as the second parameter.\n",
    "Custom splitter must add column 'group' with two possible values: 'A' or 'B'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_splitter(X: pd.DataFrame = None,\n",
    "#                 target: str = None,\n",
    "#                 split_rate: float = None) -> pd.DataFrame:\n",
    "#   # some splitter logic here\n",
    "#     pass\n",
    "\n",
    "# splitter = Splitter(split_rate=config['splitter']['split_rate'],\n",
    "#                    custom_splitter=my_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of A/B test\n",
    "\n",
    "Here\n",
    "- **alpha** — significance level\n",
    "- **beta** — probability of type II error\n",
    "- **alternative** — 'less', 'greater', 'two-sided'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = ABTest(alpha=config['hypothesis']['alpha'], \n",
    "            beta=config['hypothesis']['beta'],\n",
    "            alternative=config['hypothesis']['alternative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set loaded dataset as analyzed\n",
    "\n",
    "Here\n",
    "- **id_col** — id column of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.use_dataset(data_1, id_col=config['data']['id_col'],\n",
    "              target=config['data']['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set previously defined splitter for test\n",
    "\n",
    "Assign defined splitter to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.splitter = splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set list of split rates for MDE exploration\n",
    "\n",
    "Set a list of split rates between control/treatment you are going to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.split_rates = config['simulation']['split_rates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set list of increments for MDE exploration\n",
    "\n",
    "Here\n",
    "- **inc_var** — list of increments, i.e. [1, 2, 3, 4, 5]\n",
    "- **extra_paramms** — extra parameters for increment, currently not used in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.set_increment(inc_var=config['simulation']['increment']['vars'],\n",
    "                extra_params=config['simulation']['increment']['extra_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create metric which you want to compare\n",
    "\n",
    "In the example below, we want to compare 10th percentile of control and treatment distributions.\n",
    "Metric function must return a single value over a set of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(X: np.array) -> float:\n",
    "    return np.quantile(X, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDE simulation in order to find the best combination of split rate—increment\n",
    "\n",
    "Here\n",
    "- **n_iter** — number of iterations of simulation\n",
    "- **n_boot_samples** — set if you chose bootstrap hypothesis testing\n",
    "- **metric_type** — metric type: ratio or solid (continuous)\n",
    "- **metric** — Python function as tested metric (quantile, median, mean, etc) or custom\n",
    "- **strategy** — strategy of hypothesis testing\n",
    "- **strata** — strata column name for variance reduction\n",
    "- **strata_weights** — weights of each unique value in strata column as a dictionary\n",
    "- **to_csv** — whether or not to save the result to csv file\n",
    "- **csv_path** — path to the newly created csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ab.mde_simulation(n_iter=config['simulation']['n_iter'],\n",
    "                       metric_type=config['metric']['metric_type'],\n",
    "                       metric=metric,\n",
    "                       strategy=config['hypothesis']['strategy'],\n",
    "                       to_csv=config['result']['to_csv'],\n",
    "                       csv_path=config['result']['csv_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print simulation log\n",
    "\n",
    "Here\n",
    "- **first key** — split rate\n",
    "- **second key** — increment\n",
    "- **value** — share of rejected H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize simulation log in plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf.plot_simulation_log(config['result']['csv_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Experiment\n",
    "\n",
    "During this step, dataset of outcomes is gathered and is ready for the analysis.\n",
    "\n",
    "# 1.3. A/A test\n",
    "\n",
    "Yes, it must be run before or in parallel with A/B test, but let's assume that we have data after A/B test has finished and now we need to assure that the splitter is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aa = data[['height_now']]\n",
    "data_aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = Splitter(split_rate=0.5)\n",
    "res = splitter.aa_test(X=data_aa, target='height_now', alpha=0.05, n_iter=100)\n",
    "print(f'Share of iterations when control and treatment groups are equal: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4. Variance reduction\n",
    "\n",
    "## Loading dataset generated during A/B test\n",
    "\n",
    "Here\n",
    "- **height_now** — experiment metric during experiment\n",
    "- **height_prev** — experiment metric before experiment\n",
    "- **weight_now** — highly correlated feature with metric during experiment\n",
    "- **weight_prev** — highly correlated feature with metric before experiment\n",
    "- **noise_now** — feature during experiment that is just noise\n",
    "- **noise_prev** — feature before experiment that is just noise\n",
    "- **group** — group column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vr = data[['noise_prev', 'weight_prev', 'height_prev', 'noise_now', 'weight_now', 'height_now', 'group']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial distribution of tested metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf.plot_distributions(data_vr, 'height_now', 'group', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, distributions are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add increment to the treatment group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = ABTest(alpha=config['hypothesis']['alpha'], \n",
    "            beta=config['hypothesis']['beta'],\n",
    "            alternative=config['hypothesis']['alternative'])\n",
    "\n",
    "treatment = data_vr.loc[data_vr.group == 'B', 'height_now']\n",
    "treatment_increased = ab._add_increment('solid', treatment, 5)\n",
    "data_vr.loc[data_vr.group == 'B', 'height_now'] = treatment_increased\n",
    "\n",
    "gf.plot_distribution(treatment_increased, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial control and increased treatment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf.plot_distributions(data_vr, 'height_now', 'group', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CUPED to reduce variance\n",
    "\n",
    "After the execution, new column is introduced — **height_now_cuped**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = VarianceReduction()\n",
    "data_vr_cuped = vr.cuped(data_vr, target='height_now', groups='group', covariate='height_prev')\n",
    "print(data_vr_cuped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf.plot_distributions(data_vr_cuped, 'height_now_cuped', 'group', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, variance reduced **from 160 to 170** and **from 190 to 180** for control and **from 165 to 175** and **from 195 to 185** for treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CUPAC to reduce variance\n",
    "\n",
    "Below you can see the model that was created to predict covariate to experiment period.\n",
    "After the execution, new column is introduced — **target_pred**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vr_cupac = vr.cupac(data_vr, target_prev='height_prev', target_now='height_now',\n",
    "               factors_prev=['weight_prev'],\n",
    "               factors_now=['weight_now'], groups='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_vr_cupac.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf.plot_distributions(data_vr_cupac, 'height_now_cuped', 'group', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, variance reduced **from 160 to 170** on the left and **from 190 to 180** on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5. A/B test analysis\n",
    "\n",
    "Metric tested in the experiment in 10th quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(X: np.array) -> float:\n",
    "    return np.quantile(X, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = ABTest(alpha=config['hypothesis']['alpha'],\n",
    "            beta=config['hypothesis']['beta'],\n",
    "            alternative=config['hypothesis']['alternative'])\n",
    "\n",
    "control = data_vr_cuped.loc[data_vr_cuped.group == 'A', 'height_now_cuped'].to_numpy()\n",
    "treatment = data_vr_cuped.loc[data_vr_cuped.group == 'B', 'height_now_cuped'].to_numpy()\n",
    "\n",
    "is_rejected = ab.test_hypothesis_buckets(control, treatment, metric, n_buckets=config['hypothesis']['n_buckets'])\n",
    "result = 'rejected' if is_rejected == 1 else 'not rejected'\n",
    "print(f'H0: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ratio metric\n",
    "# 2.1. Preparation to the experiment\n",
    "## Loading dataset\n",
    "\n",
    "- **sex, married, country, height** — features\n",
    "- **clicks, sessions** — numerator and denominator of ratio metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data[['id', 'sex', 'married', 'country', 'clicks_prev', 'sessions_prev']]\n",
    "data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of splitter\n",
    "\n",
    "If you are going to run MDE simulation, **split_rate** parameter can be omitted as it will be placed to the splitter during the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = Splitter(split_rate=config['splitter']['split_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom splitter\n",
    "\n",
    "We can pass custom splitter as the second parameter.\n",
    "Custom splitter must add column 'group'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_splitter(X: pd.DataFrame = None,\n",
    "#                 target: str = None,\n",
    "#                 split_rate: float = None) -> pd.DataFrame:\n",
    "#   # some splitter logic here\n",
    "#     pass\n",
    "\n",
    "# splitter = Splitter(split_rate=config['splitter']['split_rate'],\n",
    "#                    custom_splitter=my_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of A/B test\n",
    "\n",
    "Here\n",
    "- **alpha** — significance level\n",
    "- **beta** — probability of type II error\n",
    "- **alternative** — 'less', 'more', 'two-sided'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = ABTest(alpha=config['hypothesis']['alpha'], \n",
    "            beta=config['hypothesis']['beta'],\n",
    "            alternative=config['hypothesis']['alternative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set loaded dataset as analyzed\n",
    "\n",
    "Here\n",
    "- **id_col** — id column of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.use_dataset(data_2, id_col=config['data']['id_col'],\n",
    "              numerator=config['data']['numerator'],\n",
    "              denominator=config['data']['denominator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set previously defined splitter for test\n",
    "\n",
    "Assign defined splitter to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.splitter = splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set list of split rates for MDE exploration\n",
    "\n",
    "Set a list of split rates between control/treatment you are going to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.split_rates = config['simulation']['split_rates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set list of increments for MDE exploration\n",
    "\n",
    "Here\n",
    "- **inc_var** — list of increments, i.e. [1, 2, 3, 4, 5]\n",
    "- **extra_paramms** — extra parameters for increment, currently not used in analysis\n",
    "\n",
    "**Note**: if *numerator + inc_var > denominator* then increment randomly chosen such that *numerator <= denominator*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.set_increment(inc_var=config['simulation']['increment']['vars'],\n",
    "                extra_params=config['simulation']['increment']['extra_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric to compare\n",
    "\n",
    "In most cases, \"means\" of ratio metrics are compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDE simulation in order to find the best combination of split rate—increment\n",
    "\n",
    "Here\n",
    "- **n_iter** — number of iterations of simulation\n",
    "- **n_boot_samples** — set if you chose bootstrap hypothesis testing\n",
    "- **metric_type** — metric type: ratio or solid (continuous)\n",
    "- **metric** — Python function as tested metric (quantile, median, mean, etc) or custom\n",
    "- **strategy** — strategy of hypothesis testing\n",
    "- **strata** — strata column name for variance reduction\n",
    "- **strata_weights** — weights of each unique value in strata column as a dictionary\n",
    "- **to_csv** — whether or not to save the result to csv file\n",
    "- **csv_path** — path to the newly created csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ab.mde_simulation(n_iter=config['simulation']['n_iter'],\n",
    "                       metric_type='ratio',\n",
    "                       strategy='delta_method',\n",
    "                       to_csv=True,\n",
    "                       csv_path='../data/internal/guide/ratio_mde.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print simulation log\n",
    "\n",
    "Here\n",
    "- **first key** — split rate\n",
    "- **second key** — increment\n",
    "- **value** — share of rejected H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize simulation log in plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf.plot_simulation_log('../data/internal/guide/ratio_mde.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Experiment\n",
    "\n",
    "During this step, dataset of outcomes is gathered and is ready for the analysis.\n",
    "\n",
    "# 2.3. A/A test\n",
    "\n",
    "Yes, it must be run before or in parallel with A/B test, but let's assume that we have data after A/B test has finished and now we need to assure that the splitter is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aa = data[['clicks_now', 'sessions_now']]\n",
    "data_aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = Splitter(split_rate=0.5)\n",
    "res = splitter.aa_test(X=data_aa, numerator='clicks_now', denominator='sessions_now', \n",
    "                       metric_type='ratio', alpha=0.05, n_iter=100)\n",
    "print(f'Share of iterations when control and treatment groups are equal: {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. Variance reduction\n",
    "\n",
    "Not applicable to ratio metric (as far as I know) but if ratio metric was linearized and now presented as a continuous metric then variance reduction can be easily used here as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ab = ABTest(alpha=config['hypothesis']['alpha'], \n",
    "#             beta=config['hypothesis']['beta'],\n",
    "#             alternative=config['hypothesis']['alternative'])\n",
    "\n",
    "# ab.use_dataset(some_data, id_col=config['data']['id_col'],\n",
    "#               numerator=config['data']['numerator'],\n",
    "#               denominator=config['data']['denominator'])\n",
    "\n",
    "# ab.linearization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we applied linearization, new continuous metric was added to a dataset with name **'numerator_denominator'**, where *numerator* and *denominator* are names of ratio's numerator and denominator columns.\n",
    "\n",
    "New **'numerator_denominator'** column is already added as a target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. A/B test analysis\n",
    "\n",
    "Tested metric is *ratio mean*.\n",
    "Here we back to ratio metric instead of continuous derived in previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ab = data[['id', 'clicks_now', 'sessions_now', 'group']]\n",
    "\n",
    "ab = ABTest(alpha=config['hypothesis']['alpha'],\n",
    "            beta=config['hypothesis']['beta'],\n",
    "            alternative=config['hypothesis']['alternative'])\n",
    "\n",
    "ab.use_dataset(data_ab, id_col=config['data']['id_col'],\n",
    "              numerator='clicks_now',\n",
    "              denominator='sessions_now')\n",
    "\n",
    "is_rejected = ab.delta_method()\n",
    "result = 'rejected' if is_rejected == 1 else 'not rejected'\n",
    "print(f'H0: {result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
